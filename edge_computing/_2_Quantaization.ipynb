{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# baseline"
      ],
      "metadata": {
        "id": "wJl9OTAmavWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.quantization import QuantStub, DeQuantStub, fuse_modules, prepare_qat, convert\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T797m_YHBvG",
        "outputId": "6e418be0-fac0-436c-9ef9-aab351f97345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/18: 100%|██████████| 49/49 [00:17<00:00,  2.83batch/s]\n",
            "Epoch 2/18:   0%|          | 0/49 [00:00<?, ?batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.3422739505767822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/18: 100%|██████████| 49/49 [00:14<00:00,  3.36batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 1.1376851797103882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/18: 100%|██████████| 49/49 [00:14<00:00,  3.37batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 0.8881201148033142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/18: 100%|██████████| 49/49 [00:14<00:00,  3.37batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Loss: 0.6752976179122925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/18: 100%|██████████| 49/49 [00:14<00:00,  3.37batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Loss: 0.5084292888641357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/18: 100%|██████████| 49/49 [00:14<00:00,  3.37batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Loss: 0.4594053328037262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/18: 100%|██████████| 49/49 [00:14<00:00,  3.35batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Loss: 0.4147387146949768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/18: 100%|██████████| 49/49 [00:15<00:00,  3.26batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Loss: 0.37460246682167053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/18: 100%|██████████| 49/49 [00:15<00:00,  3.19batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Loss: 0.3497352600097656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/18: 100%|██████████| 49/49 [00:14<00:00,  3.38batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 0.22750821709632874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/18: 100%|██████████| 49/49 [00:15<00:00,  3.23batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Loss: 0.23285487294197083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/18: 100%|██████████| 49/49 [00:14<00:00,  3.35batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Loss: 0.19965222477912903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/18: 100%|██████████| 49/49 [00:14<00:00,  3.32batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Loss: 0.1780196875333786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/18: 100%|██████████| 49/49 [00:14<00:00,  3.35batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Loss: 0.13473892211914062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/18: 100%|██████████| 49/49 [00:14<00:00,  3.32batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Loss: 0.09921085834503174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/18: 100%|██████████| 49/49 [00:16<00:00,  2.88batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Loss: 0.04413110762834549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/18: 100%|██████████| 49/49 [00:15<00:00,  3.14batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Loss: 0.05444111302495003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/18: 100%|██████████| 49/49 [00:14<00:00,  3.31batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Loss: 0.05702977627515793\n",
            "Original Model Training Time: 137.36 seconds\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-242f76e99880>:167: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(\"original_model.pt\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "wsPf0A29aUQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "class ComplexCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ComplexCNN, self).__init__()\n",
        "\n",
        "        # First Conv Block\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Second Conv Block\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Thrid Conv Block\n",
        "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # 4th Conv Block\n",
        "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(512)\n",
        "        self.relu7 = nn.ReLU()\n",
        "        self.conv8 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(512)\n",
        "        self.relu8 = nn.ReLU()\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # FC Layer\n",
        "        self.fc1 = nn.Linear(512 * 2 * 2, 1024)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "\n",
        "        # Quantization Stubs\n",
        "        self.quant = QuantStub()\n",
        "        self.dequant = DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Start quantization\n",
        "        x = self.quant(x)\n",
        "\n",
        "        # First Conv Block\n",
        "        x = self.pool1(self.relu2(self.bn2(self.conv2(self.relu1(self.bn1(self.conv1(x)))))))\n",
        "\n",
        "        # Second Conv Block\n",
        "        x = self.pool2(self.relu4(self.bn4(self.conv4(self.relu3(self.bn3(self.conv3(x)))))))\n",
        "\n",
        "        # Third Conv Block\n",
        "        x = self.pool3(self.relu6(self.bn6(self.conv6(self.relu5(self.bn5(self.conv5(x)))))))\n",
        "\n",
        "        # 4th Conv Block\n",
        "        x = self.pool4(self.relu8(self.bn8(self.conv8(self.relu7(self.bn7(self.conv7(x)))))))\n",
        "\n",
        "        # Flatten\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "\n",
        "        # FC Layer\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        # Dequantization\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "    def fuse_model(self):\n",
        "        # Merge BatchNorm and ReLU\n",
        "        fuse_modules(self, [\n",
        "            ['conv1', 'bn1', 'relu1'],\n",
        "            ['conv2', 'bn2', 'relu2'],\n",
        "            ['conv3', 'bn3', 'relu3'],\n",
        "            ['conv4', 'bn4', 'relu4'],\n",
        "            ['conv5', 'bn5', 'relu5'],\n",
        "            ['conv6', 'bn6', 'relu6'],\n",
        "            ['conv7', 'bn7', 'relu7'],\n",
        "            ['conv8', 'bn8', 'relu8']\n",
        "        ], inplace=True)\n"
      ],
      "metadata": {
        "id": "GVbu2Wn8ab2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = ComplexCNN()\n",
        "\n",
        "# Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "\n",
        "# Model, Loss, Optimizer\n",
        "model = ComplexCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n"
      ],
      "metadata": {
        "id": "dGTjOP7Hag1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-QAT Training\n",
        "model.train()\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(18):\n",
        "    for data, target in tqdm(train_loader, desc=f\"Epoch {epoch+1}/18\", unit=\"batch\"):\n",
        "        data, target = data.to(device), target.to(device)  # 데이터를 GPU로 이동\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "end_time = time.time()\n",
        "print(f\"Original Model Training Time: {(end_time - start_time) / 2:.2f} seconds\")\n",
        "\n",
        "torch.save(model, \"original_model.pt\")\n"
      ],
      "metadata": {
        "id": "Kk_EavBrakPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for inference\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to('cpu'), target.to('cpu')\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "-FRP0UI-amuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset and DataLoader for Evaluation\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
        "\n",
        "# Load the original model\n",
        "model = torch.load(\"original_model.pt\")\n",
        "model.to('cpu')\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = evaluate_model(model, test_loader)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "wNw7M09Kaoaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PTQ, QAT 적용 및 평가"
      ],
      "metadata": {
        "id": "RTH3gh8Qa00o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.quantization import QuantStub, DeQuantStub, fuse_modules, prepare_qat, convert, quantize_dynamic\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import contextlib\n",
        "import random\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "logTb7rqHCne",
        "outputId": "ed253f1a-168b-4bac-8a63-20fe8fd97ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Starting Quantization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-99eaf9410049>:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  original_model = torch.load(\"original_model.pt\", map_location=device)\n",
            "<ipython-input-12-99eaf9410049>:157: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ptq_model = torch.load(\"original_model.pt\", map_location='cpu')\n",
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "PTQ Calibration: 100%|██████████| 5/5 [01:10<00:00, 14.03s/batch]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibration Time: 83.01 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-99eaf9410049>:192: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  qat_model = torch.load(\"original_model.pt\", map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "QAT Fine-tuning: 100%|██████████| 49/49 [00:20<00:00,  2.41batch/s]\n",
            "QAT Fine-tuning:   0%|          | 0/49 [00:00<?, ?batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "QAT Fine-tuning: 100%|██████████| 49/49 [00:19<00:00,  2.54batch/s]\n",
            "QAT Fine-tuning:   0%|          | 0/49 [00:00<?, ?batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "QAT Fine-tuning: 100%|██████████| 49/49 [00:19<00:00,  2.54batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantization completed. Starting evaluation iterations...\n",
            "Iteration 1/5\n",
            "Iteration 1 ptq model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-99eaf9410049>:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  original_model = torch.load(\"original_model.pt\", map_location='cpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference time: 47.834423303604126, Accuracy: 75.08\n",
            "Iteration 1 qat model\n",
            "Inference time: 46.7501118183136, Accuracy: 85.98\n",
            "Iteration 1 original model\n",
            "Inference time: 111.24269914627075, Accuracy: 78.99\n",
            "Iteration 2/5\n",
            "Iteration 2 ptq model\n",
            "Inference time: 45.49363327026367, Accuracy: 75.08\n",
            "Iteration 2 qat model\n",
            "Inference time: 48.4896445274353, Accuracy: 85.98\n",
            "Iteration 2 original model\n",
            "Inference time: 111.10444259643555, Accuracy: 78.99\n",
            "Iteration 3/5\n",
            "Iteration 3 ptq model\n",
            "Inference time: 46.862894773483276, Accuracy: 75.08\n",
            "Iteration 3 qat model\n",
            "Inference time: 46.61470437049866, Accuracy: 85.98\n",
            "Iteration 3 original model\n",
            "Inference time: 113.87418723106384, Accuracy: 78.99\n",
            "Iteration 4/5\n",
            "Iteration 4 ptq model\n",
            "Inference time: 48.674333810806274, Accuracy: 75.08\n",
            "Iteration 4 qat model\n",
            "Inference time: 47.91838598251343, Accuracy: 85.98\n",
            "Iteration 4 original model\n",
            "Inference time: 117.29656553268433, Accuracy: 78.99\n",
            "Iteration 5/5\n",
            "Iteration 5 ptq model\n",
            "Inference time: 46.46167063713074, Accuracy: 75.08\n",
            "Iteration 5 qat model\n",
            "Inference time: 46.61030578613281, Accuracy: 85.98\n",
            "Iteration 5 original model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4f0nRTGja-KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "class ComplexCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ComplexCNN, self).__init__()\n",
        "\n",
        "        # First Conv Block\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Second Conv Block\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Thrid Conv Block\n",
        "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # 4th Conv Block\n",
        "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn7 = nn.BatchNorm2d(512)\n",
        "        self.relu7 = nn.ReLU()\n",
        "        self.conv8 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn8 = nn.BatchNorm2d(512)\n",
        "        self.relu8 = nn.ReLU()\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # FC Layer\n",
        "        self.fc1 = nn.Linear(512 * 2 * 2, 1024)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "\n",
        "        # Quantization Stubs\n",
        "        self.quant = QuantStub()\n",
        "        self.dequant = DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Start quantization\n",
        "        x = self.quant(x)\n",
        "\n",
        "        # First Conv Block\n",
        "        x = self.pool1(self.relu2(self.bn2(self.conv2(self.relu1(self.bn1(self.conv1(x)))))))\n",
        "\n",
        "        # Second Conv Block\n",
        "        x = self.pool2(self.relu4(self.bn4(self.conv4(self.relu3(self.bn3(self.conv3(x)))))))\n",
        "\n",
        "        # Third Conv Block\n",
        "        x = self.pool3(self.relu6(self.bn6(self.conv6(self.relu5(self.bn5(self.conv5(x)))))))\n",
        "\n",
        "        # 4th Conv Block\n",
        "        x = self.pool4(self.relu8(self.bn8(self.conv8(self.relu7(self.bn7(self.conv7(x)))))))\n",
        "\n",
        "        # Flatten\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "\n",
        "        # FC Layer\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        # Dequantization\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "    def fuse_model(self):\n",
        "        # Merge BatchNorm and ReLU\n",
        "        fuse_modules(self, [\n",
        "            ['conv1', 'bn1', 'relu1'],\n",
        "            ['conv2', 'bn2', 'relu2'],\n",
        "            ['conv3', 'bn3', 'relu3'],\n",
        "            ['conv4', 'bn4', 'relu4'],\n",
        "            ['conv5', 'bn5', 'relu5'],\n",
        "            ['conv6', 'bn6', 'relu6'],\n",
        "            ['conv7', 'bn7', 'relu7'],\n",
        "            ['conv8', 'bn8', 'relu8']\n",
        "        ], inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "NcH-u_gJJdSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = ComplexCNN().to(device)\n",
        "model.eval()\n",
        "\n",
        "# Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "Yi9LEHuJbFN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to measure inference time and accuracy\n",
        "def measure_performance(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to('cpu'), target.to('cpu')\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    print(f'Inference time: {inference_time}, Accuracy: {accuracy}')\n",
        "    return inference_time, accuracy\n"
      ],
      "metadata": {
        "id": "Em0VwlDBbH4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Quantization\n",
        "print(\"Starting Quantization...\")\n",
        "\n",
        "# Load the original pre-trained model\n",
        "original_model = torch.load(\"original_model.pt\", map_location=device)\n",
        "\n",
        "# PTQ: Post-Training Quantization\n",
        "ptq_model = torch.load(\"original_model.pt\", map_location='cpu')\n",
        "\n"
      ],
      "metadata": {
        "id": "S_q930fjbMOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calibration (Static Quantization)\n",
        "ptq_model.eval()\n",
        "ptq_model.fuse_model()  # Fuse layers\n",
        "ptq_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "torch.quantization.prepare(ptq_model, inplace=True)\n",
        "\n",
        "def create_balanced_subset(dataset, num_samples_per_class):\n",
        "    class_indices = defaultdict(list)\n",
        "\n",
        "    for idx, (_, label) in enumerate(dataset):\n",
        "        class_indices[label].append(idx)\n",
        "\n",
        "    selected_indices = []\n",
        "    for cls, indices in class_indices.items():\n",
        "        selected_indices.extend(random.sample(indices, num_samples_per_class))\n",
        "\n",
        "    subset = torch.utils.data.Subset(dataset, selected_indices)\n",
        "    return subset\n",
        "\n",
        "start_calibration = time.time()\n",
        "calibration_subset = create_balanced_subset(train_dataset, num_samples_per_class=500)\n",
        "calibration_loader = torch.utils.data.DataLoader(calibration_subset, batch_size=1024, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZEOPsR_RbQEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the test dataset for calibration\n",
        "with torch.no_grad():\n",
        "    for data, _ in tqdm(calibration_loader, desc='PTQ Calibration', unit='batch'):\n",
        "        ptq_model(data)\n",
        "end_calibration = time.time()\n",
        "print(f'Calibration Time: {end_calibration - start_calibration:.2f} seconds')\n",
        "\n",
        "torch.quantization.convert(ptq_model, inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "-dWTpgjdbTCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QAT: Quantization-Aware Training\n",
        "qat_model = torch.load(\"original_model.pt\", map_location=device)\n",
        "qat_model.eval()\n",
        "qat_model.fuse_model()  # Fuse the layers before training\n",
        "qat_model.train()  # Switch back to train mode for QAT\n",
        "\n",
        "qat_qconfig = torch.quantization.QConfig(\n",
        "    activation=torch.quantization.FakeQuantize.with_args(\n",
        "        observer=torch.quantization.MovingAverageMinMaxObserver,\n",
        "        quant_min=0,\n",
        "        quant_max=255,\n",
        "        dtype=torch.quint8,\n",
        "        qscheme=torch.per_tensor_affine\n",
        "    ),\n",
        "    weight=torch.quantization.FakeQuantize.with_args(\n",
        "        observer=torch.quantization.MovingAverageMinMaxObserver,\n",
        "        quant_min=-128,\n",
        "        quant_max=127,\n",
        "        dtype=torch.qint8,\n",
        "        qscheme=torch.per_tensor_affine\n",
        "    )\n",
        ")\n",
        "\n",
        "qat_model.qconfig = qat_qconfig\n",
        "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
        "\n",
        "optimizer = optim.Adam(qat_model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "hG91L3SabVdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 epoch fine-tuning for QAT\n",
        "for i in range(3):\n",
        "    print(f'Epoch {i+1}:')\n",
        "    for data, target in tqdm(train_loader, desc=\"QAT Fine-tuning\", unit=\"batch\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = qat_model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "qat_model.eval()\n",
        "qat_model.to('cpu')  # Move to CPU before quantization\n",
        "convert(qat_model, inplace=True)\n",
        "\n",
        "print(\"Quantization completed. Starting evaluation iterations...\")\n",
        "\n"
      ],
      "metadata": {
        "id": "epG6TJFKbYWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the original pre-trained model for each iteration\n",
        "original_model = torch.load(\"original_model.pt\", map_location='cpu')\n",
        "original_model.eval()\n",
        "\n"
      ],
      "metadata": {
        "id": "MQqdObo6bcdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Iterations\n",
        "results = []\n",
        "for i in range(5):\n",
        "    print(f\"Iteration {i+1}/5\")\n",
        "\n",
        "    print(f'Iteration {i+1} ptq model')\n",
        "    ptq_time, ptq_accuracy = measure_performance(ptq_model, test_loader)\n",
        "    print(f'Iteration {i+1} qat model')\n",
        "    qat_time, qat_accuracy = measure_performance(qat_model, test_loader)\n",
        "    print(f'Iteration {i+1} original model')\n",
        "    original_time, original_accuracy = measure_performance(original_model, test_loader)\n",
        "\n",
        "    results.append({\n",
        "        \"Iteration\": i + 1,\n",
        "        \"Original_Inference_Time\": original_time,\n",
        "        \"Original_Accuracy\": original_accuracy,\n",
        "        \"PTQ_Inference_Time\": ptq_time,\n",
        "        \"PTQ_Accuracy\": ptq_accuracy,\n",
        "        \"QAT_Inference_Time\": qat_time,\n",
        "        \"QAT_Accuracy\": qat_accuracy,\n",
        "    })\n"
      ],
      "metadata": {
        "id": "Ssah8ZOubhJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert results to DataFrame and compute statistics\n",
        "df = pd.DataFrame(results)\n",
        "df['Original_Inference_Time_Mean'] = df['Original_Inference_Time'].mean()\n",
        "df['Original_Accuracy_Mean'] = df['Original_Accuracy'].mean()\n",
        "df['PTQ_Inference_Time_Mean'] = df['PTQ_Inference_Time'].mean()\n",
        "df['PTQ_Accuracy_Mean'] = df['PTQ_Accuracy'].mean()\n",
        "df['QAT_Inference_Time_Mean'] = df['QAT_Inference_Time'].mean()\n",
        "df['QAT_Accuracy_Mean'] = df['QAT_Accuracy'].mean()\n",
        "\n",
        "df['Original_Inference_Time_Max'] = df['Original_Inference_Time'].max()\n",
        "df['Original_Accuracy_Max'] = df['Original_Accuracy'].max()\n",
        "df['PTQ_Inference_Time_Max'] = df['PTQ_Inference_Time'].max()\n",
        "df['PTQ_Accuracy_Max'] = df['PTQ_Accuracy'].max()\n",
        "df['QAT_Inference_Time_Max'] = df['QAT_Inference_Time'].max()\n",
        "df['QAT_Accuracy_Max'] = df['QAT_Accuracy'].max()\n",
        "\n",
        "df['Original_Inference_Time_Min'] = df['Original_Inference_Time'].min()\n",
        "df['Original_Accuracy_Min'] = df['Original_Accuracy'].min()\n",
        "df['PTQ_Inference_Time_Min'] = df['PTQ_Inference_Time'].min()\n",
        "df['PTQ_Accuracy_Min'] = df['PTQ_Accuracy'].min()\n",
        "df['QAT_Inference_Time_Min'] = df['QAT_Inference_Time'].min()\n",
        "#df['QAT_Accuracy_Min'] = df['QAT_Accuracy_Min'].min()"
      ],
      "metadata": {
        "id": "ol3GWb7abo2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to CSV\n",
        "df.to_csv(\"quantization_performance.csv\", index=False)\n",
        "print(\"Results saved to quantization_performance.csv\")"
      ],
      "metadata": {
        "id": "kCL6azwvbr8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}